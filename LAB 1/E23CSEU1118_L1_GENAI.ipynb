{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No5-UJRUd5SX"
   },
   "source": [
    "#Mounting google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "src = \"/content/drive/MyDrive/cat_species\"\n",
    "dst = \"/content/cat_species\"\n",
    "\n",
    "if os.path.exists(dst):\n",
    "    shutil.rmtree(dst)\n",
    "\n",
    "shutil.copytree(src, dst)\n",
    "\n",
    "print(\"Folder copied from Drive to Colab.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhqtDsRseX7c"
   },
   "source": [
    "#Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ftfy regex tqdm\n",
    "!pip install -q git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install diffusers transformers accelerate torch torchvision pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4-5kwG9ejjR"
   },
   "source": [
    "#Loading Diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSObkRy4fKZa"
   },
   "source": [
    "#Generating Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_species = [\n",
    "    \"Abyssinian\", \"American Bobtail\", \"American Curl\", \"American Shorthair\",\n",
    "    \"Balinese\", \"Bengal\", \"Birman\", \"Bombay\",\n",
    "    \"British Shorthair\", \"Burmese\", \"Chartreux\", \"Cornish Rex\",\n",
    "    \"Devon Rex\", \"Egyptian Mau\", \"Exotic Shorthair\", \"Himalayan\",\n",
    "    \"Japanese Bobtail\", \"Korat\", \"LaPerm\", \"Maine Coon\",\n",
    "    \"Manx\", \"Norwegian Forest\", \"Ocicat\", \"Oriental Shorthair\",\n",
    "    \"Persian\", \"Peterbald\", \"Ragdoll\", \"Russian Blue\",\n",
    "    \"Savannah\", \"Scottish Fold\", \"Selkirk Rex\", \"Siamese\",\n",
    "    \"Siberian\", \"Singapura\", \"Somali\", \"Sphynx\",\n",
    "    \"Tonkinese\", \"Toyger\", \"Turkish Angora\", \"Turkish Van\",\n",
    "    \"Snowshoe\", \"Burmilla\"\n",
    "]\n",
    "print(len(cat_species))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"cat_species\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "print(\"Parent folder created:\", base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in cat_species:\n",
    "    folder_path = os.path.join(base_dir, species)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "print(\"All 42 species folders created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PER_LABEL = 10\n",
    "NUM_STEPS = 20\n",
    "GUIDANCE_SCALE = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in os.listdir(base_dir):\n",
    "    species_path = os.path.join(base_dir, species)\n",
    "\n",
    "    if not os.path.isdir(species_path):\n",
    "        continue\n",
    "\n",
    "    prompt = f\"A high quality realistic photo of a {species.replace('_', ' ')} cat\"\n",
    "\n",
    "    for i in range(IMAGES_PER_LABEL):\n",
    "        image = pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=NUM_STEPS,\n",
    "            guidance_scale=GUIDANCE_SCALE\n",
    "        ).images[0]\n",
    "\n",
    "        image_name = f\"{species}_{i+1}.png\"\n",
    "        image.save(os.path.join(species_path, image_name))\n",
    "\n",
    "    print(f\"Generated {IMAGES_PER_LABEL} images for {species}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTLArjpYfZoY"
   },
   "source": [
    "#Classifying images with pre-trained Resnet-50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "image_dir = \"cat_species_all_images\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "image_dir = \"cat_species\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "valid_cat_keywords = [\n",
    "    \"cat\", \"tabby\", \"tiger\", \"persian\", \"egyptian\", \"lynx\"\n",
    "]\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "imagenet_classes = ResNet50_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
    "\n",
    "for species in os.listdir(image_dir):\n",
    "    species_path = os.path.join(image_dir, species)\n",
    "\n",
    "    if not os.path.isdir(species_path):\n",
    "        continue\n",
    "\n",
    "    for img_name in os.listdir(species_path):\n",
    "        if not img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(species_path, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            pred_idx = output.argmax(dim=1).item()\n",
    "\n",
    "        label = imagenet_classes[pred_idx].lower()\n",
    "\n",
    "        if any(k in label for k in valid_cat_keywords):\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"Proxy accuracy (cat detection): {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ctTtmwFficb"
   },
   "source": [
    "#Importing libraries for Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygfypNVfzVo"
   },
   "source": [
    "#Custom Residual CNN with Squeeze-and-Excitation Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, r=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // r)\n",
    "        self.fc2 = nn.Linear(channels // r, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        y = x.mean((2,3))\n",
    "        y = torch.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class ResidualSEBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "        self.se = SEBlock(out_ch)\n",
    "\n",
    "        self.skip = nn.Identity() if in_ch == out_ch and stride == 1 else \\\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.se(self.conv(x)) + self.skip(x))\n",
    "\n",
    "\n",
    "class ResidualSE_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2, 1)\n",
    "        )\n",
    "\n",
    "        self.l1 = ResidualSEBlock(64, 64)\n",
    "        self.l2 = ResidualSEBlock(64, 128, 2)\n",
    "        self.l3 = ResidualSEBlock(128, 256, 2)\n",
    "        self.l4 = ResidualSEBlock(256, 512, 2)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.gap(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def get_features(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.gap(x).flatten(1)\n",
    "        return F.normalize(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfkN1uGdf66F"
   },
   "source": [
    "#Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(data_dir, batch_size=32, img_size=224):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    indices = np.random.permutation(len(dataset))\n",
    "\n",
    "    n = len(indices)\n",
    "    train_idx = indices[:int(0.7*n)]\n",
    "    val_idx   = indices[int(0.7*n):int(0.85*n)]\n",
    "    test_idx  = indices[int(0.85*n):]\n",
    "\n",
    "    train_loader = DataLoader(Subset(dataset, train_idx),\n",
    "                              batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(Subset(dataset, val_idx),\n",
    "                            batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(Subset(dataset, test_idx),\n",
    "                             batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, dataset.classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0XEkUZ2gBiE"
   },
   "source": [
    "#Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=30, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # -------- Training --------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            train_correct += (preds == y).sum().item()\n",
    "            train_total += y.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "        # -------- Validation --------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-SqpAEqgFwR"
   },
   "source": [
    "#Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            pred = model(x).argmax(1).cpu()\n",
    "            y_pred.extend(pred.numpy())\n",
    "            y_true.extend(y.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"Test Accuracy:\", acc*100)\n",
    "    print(\"Macro F1:\", f1)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SlIX2VfgId6"
   },
   "source": [
    "#Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_learning(model, dataset, n_way=5, k_shot=5, n_query=5):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    class_map = defaultdict(list)\n",
    "    for i in range(len(dataset)):\n",
    "        _, y = dataset[i]\n",
    "        class_map[y].append(i)\n",
    "\n",
    "    classes = np.random.choice(list(class_map.keys()), n_way, replace=False)\n",
    "\n",
    "    support, query, q_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, c in enumerate(classes):\n",
    "            idxs = np.random.choice(class_map[c], k_shot+n_query, replace=False)\n",
    "            for j in idxs[:k_shot]:\n",
    "                support.append(model.get_features(dataset[j][0].unsqueeze(0).to(device)))\n",
    "            for j in idxs[k_shot:]:\n",
    "                query.append(model.get_features(dataset[j][0].unsqueeze(0).to(device)))\n",
    "                q_labels.append(i)\n",
    "\n",
    "    support = torch.cat(support)\n",
    "    query = torch.cat(query)\n",
    "    q_labels = torch.tensor(q_labels).to(device)\n",
    "\n",
    "    prototypes = torch.stack([\n",
    "        support[i*k_shot:(i+1)*k_shot].mean(0)\n",
    "        for i in range(n_way)\n",
    "    ])\n",
    "\n",
    "    preds = torch.cdist(query, prototypes).argmin(1)\n",
    "    return (preds == q_labels).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iugi7ZUBgMne"
   },
   "source": [
    "#Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_clip(data_dir):\n",
    "    import clip, os\n",
    "    from PIL import Image\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model.eval()\n",
    "\n",
    "    classes = sorted(os.listdir(data_dir))\n",
    "    prompts = [f\"a photo of a {c.replace('_',' ')} cat\" for c in classes]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_feat = model.encode_text(clip.tokenize(prompts).to(device))\n",
    "        text_feat /= text_feat.norm(dim=1, keepdim=True)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for i, c in enumerate(classes):\n",
    "        for img in os.listdir(f\"{data_dir}/{c}\"):\n",
    "            image = preprocess(Image.open(f\"{data_dir}/{c}/{img}\").convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                img_feat = model.encode_image(image)\n",
    "                img_feat /= img_feat.norm(dim=1, keepdim=True)\n",
    "                pred = (img_feat @ text_feat.T).argmax().item()\n",
    "            correct += int(pred == i)\n",
    "            total += 1\n",
    "\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqcdqPWEgPM5"
   },
   "source": [
    "#Continual Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continual_learning_ewc(model, dataset, classes, num_tasks=3, lambda_ewc=2000):\n",
    "    device = next(model.parameters()).device\n",
    "    class_map = defaultdict(list)\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        _, y = dataset[i]\n",
    "        class_map[y].append(i)\n",
    "\n",
    "    fisher, opt_params = {}, {}\n",
    "    accs = []\n",
    "\n",
    "    task_size = len(classes) // num_tasks\n",
    "\n",
    "    for t in range(num_tasks):\n",
    "        task_classes = classes[t*task_size:(t+1)*task_size]\n",
    "        idxs = sum([class_map[c] for c in task_classes], [])\n",
    "\n",
    "        loader = DataLoader(Subset(dataset, idxs), batch_size=32, shuffle=True)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "        for _ in range(5):\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                opt.zero_grad()\n",
    "                loss = F.cross_entropy(model(x), y)\n",
    "                if fisher:\n",
    "                    loss += lambda_ewc * sum(\n",
    "                        (fisher[n] * (p - opt_params[n]).pow(2)).sum()\n",
    "                        for n,p in model.named_parameters()\n",
    "                    )\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        fisher_new = {n: torch.zeros_like(p) for n,p in model.named_parameters()}\n",
    "        opt_params = {n: p.clone() for n,p in model.named_parameters()}\n",
    "\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model.zero_grad()\n",
    "            F.nll_loss(F.log_softmax(model(x),1), y).backward()\n",
    "            for n,p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher_new[n] += p.grad.pow(2)\n",
    "\n",
    "        fisher = {n: fisher.get(n,0)+fisher_new[n]/len(loader) for n in fisher_new}\n",
    "\n",
    "        correct, total = 0, 0\n",
    "        for c in classes[:(t+1)*task_size]:\n",
    "            for i in class_map[c][:20]:\n",
    "                x,y = dataset[i]\n",
    "                pred = model(x.unsqueeze(0).to(device)).argmax(1).item()\n",
    "                correct += int(pred == y)\n",
    "                total += 1\n",
    "\n",
    "        accs.append(100 * correct / total)\n",
    "\n",
    "    return model, accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaSt7E08gR1j"
   },
   "source": [
    "#Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/content/cat_species\"\n",
    "\n",
    "train_loader, val_loader, test_loader, classes = setup_data(data_dir)\n",
    "\n",
    "model = ResidualSE_CNN(num_classes=len(classes))\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs=30)\n",
    "\n",
    "test_acc, test_f1 = evaluate_model(model, test_loader)\n",
    "\n",
    "dataset = ImageFolder(data_dir, transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "]))\n",
    "\n",
    "few_shot_acc = few_shot_learning(model, dataset)\n",
    "zero_shot_acc = zero_shot_clip(data_dir)\n",
    "\n",
    "model_cl = ResidualSE_CNN(len(classes))\n",
    "model_cl.load_state_dict(model.state_dict())\n",
    "model_cl, cl_accs = continual_learning_ewc(model_cl, dataset, list(range(len(classes))))\n",
    "\n",
    "print(\"\\nSUMMARY\")\n",
    "print(\"Supervised Acc:\", test_acc*100)\n",
    "print(\"Few-shot Acc:\", few_shot_acc*100)\n",
    "print(\"Zero-shot Acc:\", zero_shot_acc*100)\n",
    "print(\"Continual Acc:\", sum(cl_accs)/len(cl_accs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7YrLS39g60z"
   },
   "source": [
    "#Results\n",
    "The custom Res-SE-CNN trained from scratch achieved 15.87% supervised accuracy with a macro F1-score of 0.12. Few-shot learning significantly improved performance to 84.0% accuracy, while zero-shot learning achieved 42.14% accuracy. Continual learning using EWC resulted in an average accuracy of 13.29%, highlighting the impact of catastrophic forgetting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
